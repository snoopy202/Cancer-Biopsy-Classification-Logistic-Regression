{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HP9U_AO8stAq",
        "CwIq3r8RENiw"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP9U_AO8stAq"
      },
      "source": [
        "## Introduction to Breast Cancer Biopsy Classification\n",
        "\n",
        "In this project, imagine that your colleague, an oncologist (cancer doctor), is working in a major hospital that specializes in treating breast cancers. Breast cancer tumors are very complicated at the cellular level, and this makes determining whether a patient's tumor is malignant (dangerous) or benign (not dangerous) a challenge. Your task will be to build a classifier that can determine whether a sample is malignant or benign to help your colleague!\n",
        "\n",
        "Every patient that arrives at the hospital undergoes a biopsy of their tumor. This means that a small sample of the tumor is taken from the patient and various metrics are recorded about it, including: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension.\n",
        "\n",
        "Using a large dataset of labeled biopsy samples from breast cancer tumors, you will build your binary classification model to determine whether a tumor is malignant or benign based on these features. Then, this model can help you to better determine diagnoses for new patients who arrive at the hospital.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwIq3r8RENiw"
      },
      "source": [
        "## Today...\n",
        "We will explore together the steps that you could take to help your friend solve this problem!\n",
        "\n",
        "**Background and data exploration**\n",
        "\n",
        "- Exploring the data\n",
        "- Visualizing the data\n",
        "\n",
        "**Predicting Diagnosis: Working up to Logistic Regression**\n",
        "\n",
        "- Approach 1: Linear Regression classifier\n",
        "\n",
        "- Approach 2: Simple boundary classifier\n",
        "\n",
        "- Approach 3: Modifying with logistic regression\n",
        "\n",
        "- Approach 4: Multiple feature logistic regression\n",
        "\n",
        "**Bonus Discussion: What makes a separation good?**\n",
        "\n",
        "**Optional: Decision trees walkthrough**\n",
        "\n",
        "**Advanced (Optional): Choosing a Classifier**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5WlU9mlAtTt"
      },
      "source": [
        "# Background and data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOTZCNwKs8Bc"
      },
      "source": [
        "## Diagnosing cancer with biopsies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei8za5s3tVw8"
      },
      "source": [
        "**Before** we dive into building a classifier for breast cancer tumors, we will first discuss how the data are generated and what the various features mean.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%202b%20-%20Logistic%20Regression/BreastCells.jpg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "The above image is an example of cancerous (malignant) breast cells next to benign cells. These cells are part of a tumor biopsy where the extracted tissue is sampled with a special needle. The cells are subsequently stained with different dyes to help visualize their shapes, quantity of DNA, etc. These properties provide clues and insight into the rate of cell division (and remember that rapid cell division = cancerous).\n",
        "### (Optional) Data Feature Descriptions\n",
        "\n",
        "Our dataset reports 10 different features of the biopsies. Here's what a few of them mean:\n",
        "\n",
        "1. <u><b><i>Perimeter</u></b></i>: Total distance between points defining the cell's nuclear perimeter.\n",
        "2. <u><b><i>Radius</u></b></i>: Average distance from the center of the cell's nucleus to its perimeter.\n",
        "3. <u><b><i>Texture</u></b></i>: The texture of the cell nucleus is measured by finding the variance of the gray scale intensities in the component pixels.\n",
        "4. <u><b><i>Area</u></b></i>: Nuclear area is measured by counting the number of pixels on the interior of the nucleus and adding one-half of the pixels in the perimeter.\n",
        "\n",
        "The following image should give a visual to what these cell nucleus features look like:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%202b%20-%20Logistic%20Regression/Perimeter.png\">\n",
        "</center>\n",
        "\n",
        "5. <u><b><i>Smoothness</u></b></i>: Measures the smoothness of a nuclear contour by measuring the difference between the length of a radial line and the mean length of the lines surrounding it. The image below demonstrates this:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%202b%20-%20Logistic%20Regression/Smoothness.png\">\n",
        "</center>\n",
        "\n",
        "6. <u><b><i>Concavity</u></b></i>: Measures the severity of concavities or indentations in a cell nucleus. Chords are drawn between non-adjacent snake points and measure the extent to which the actual boundary lies inside each chord. The line in bold in the image below is an example of a chord.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%202b%20-%20Logistic%20Regression/Concavity.png\">\n",
        "</center>\n",
        "\n",
        "7. <u><b><i>Symmetry</u></b></i>: The major axis (longest chord) through the center is found. Then, the difference between the distance on both sides of the lines that are perpendicular to the major axis is calculated. The image below shows an example of this:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%202b%20-%20Logistic%20Regression/Symmetry.png\">\n",
        "</center>\n",
        "\n",
        "\n",
        "The paper that first detailed these measurements for this dataset can be found here for more information: https://pdfs.semanticscholar.org/1c4a/4db612212a9d3806a848854d20da9ddd0504.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYeVd9eptE31"
      },
      "source": [
        "## Breast cancer diagnostic dataset\n",
        "\n",
        "The dataset we will use to train our model is called the Breast Cancer Wisconsin (Diagnostic) Data Set. It consists of 569 biopsy samples, just like the ones described above, from breast cancer tumors.\n",
        "\n",
        "Each biopsy sample in the dataset is labeled with an ID number and whether or not the tumor it came from is malignant (1) or benign (0). Each sample also has 10 different features associated with it, some of which are described above. Remember that each feature value for a given biopsy sample is a real-valued number.\n",
        "\n",
        "Think: what sorts of features would you expect to be different between a rapidly growing, malignant cancer cell and a healthy one? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTLBQUJZJpEU"
      },
      "source": [
        "#@title Run this to download your data! { display-mode: \"form\" }\n",
        "# Load the data!\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%202b%20-%20Logistic%20Regression/cancer.csv\"\n",
        "\n",
        "data = pd.read_csv('cancer.csv')\n",
        "data['diagnosis'].replace({'M':1, 'B':0}, inplace = True)\n",
        "data.to_csv('cancer.csv')\n",
        "del data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSpUGH4NvaUk"
      },
      "source": [
        "## Loading our annotated dataset\n",
        "\n",
        "The first step in building our breast cancer tumor classification model is to load in the dataset we'll use to \"teach\" (or \"train\") our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmyUG5GrvdPf"
      },
      "source": [
        "# First, import helpful Python tools for loading/navigating data\n",
        "import os             # Good for navigating your computer's files\n",
        "import numpy as np    # Great for lists (arrays) of numbers\n",
        "import pandas as pd   # Great for tables (google spreadsheets, microsoft excel, csv)\n",
        "from sklearn.metrics import accuracy_score   # Great for creating quick ML models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cu2tGYpv5AZ"
      },
      "source": [
        "# This is the name of our data file, which was downloaded in the set up cell.\n",
        "# Check out the file explorer (folder on the left toolbar) to see where that lives!\n",
        "data_path = 'cancer.csv'\n",
        "\n",
        "# Use the 'pd.read_csv(filepath)' function to read in read our data and store it\n",
        "# in a variable called 'dataframe'\n",
        "dataframe = pd.read_csv(data_path)\n",
        "\n",
        "# Redefine `dataframe` to include only the columns discussed\n",
        "dataframe = dataframe[['diagnosis', 'perimeter_mean', 'radius_mean', 'texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean', 'symmetry_mean']]\n",
        "\n",
        "# Define a new, more descriptive `diagnosis_cat` column\n",
        "dataframe['diagnosis_cat'] = dataframe['diagnosis'].astype('category').map({1: '1 (malignant)', 0: '0 (benign)'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twcYYuhGgLda"
      },
      "source": [
        "# Exploring our data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKx4euGqwHpS"
      },
      "source": [
        " ## Looking at our dataset\n",
        "\n",
        " A key step in machine learning (and coding in general!) is to view the structure and dimensions of our new dataframe, which stores all our training data from the tumor biopsies. You can think of dataframes like Google or Microsoft Excel spreadsheets (large tables with row/column headers).\n",
        "\n",
        "We want to confirm that the size of our table is correct, check out the features present, and get a more visual sense of what it looks like overall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44xNblKYVRHh"
      },
      "source": [
        "### ‚úç Exercise\n",
        "\n",
        "Use the [`.head()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) method to show the first five rows of the table and their corresponding column headers (our biopsy features!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKRxaP1qweIz"
      },
      "source": [
        "# YOUR CODE HERE:\n",
        "\n",
        "# END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKMsdWy4J023"
      },
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tilXEPhVwkjk"
      },
      "source": [
        "Our colleague has given us documentation on what each feature column means. Specifically:\n",
        "\n",
        "* <u><b><i>diagnosis</u></b></i>: Whether the tumor was diagnosed as malignant (1) or benign (0).\n",
        "* <u><b><i>perimeter_mean</u></b></i>: The average perimeter of cells in that particular biopsy\n",
        "* <u><b><i>radius_mean</u></b></i>: The average radius of cells in that particular biopsy\n",
        "* <u><b><i>texture_mean</u></b></i>: The average texture of cells in that particular biopsy\n",
        "* <u><b><i>area_mean</u></b></i>: The average area of cells in that particular biopsy\n",
        "* <u><b><i>smoothness_mean</u></b></i>: The average smoothness of cells in that particular biopsy\n",
        "* <u><b><i>concavity_mean</u></b></i>: The average concavity of cells in that particular biopsy\n",
        "* <u><b><i>symmetry_mean</u></b></i>: The average symmetry of cells in that particular biopsy\n",
        "\n",
        "Recall that the term mean refers to taking an average (summing the values for each cell and dividing by the total number of cells observed in that biopsy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-SoD9EQw5Ie"
      },
      "source": [
        "# Next, we'll use the 'info' method to see the data types of each column\n",
        "dataframe.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM41uioFw_Ow"
      },
      "source": [
        "### üí° Discussion Question:\n",
        "\n",
        "Which columns are numeric? Why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution\n",
        "\n",
        "<details><summary> click to reveal! </summary>\n",
        "\n",
        "perimeter_mean, radius_mean, texture_mean, area_mean, smoothness_mean, concavity_mean, and symmetry_mean are all numeric. These represent the average of cell attributes across the entire biopsy sample. Note that diagnosis is technically stored as a number (int64), but it would be considered categorical."
      ],
      "metadata": {
        "id": "r-G4TMID40Rb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMJy1OXUmqnO"
      },
      "source": [
        " ## Visualizing our dataset\n",
        "\n",
        "How can we determine the relationship between each of the \"features\" of these cells and the diagnosis?\n",
        "\n",
        "The best way is to graph certain features in our data and see how they vary between different diagnoses! We will use some Python libraries like Seaborn and Matplotlib to make this an easier task for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fg1ZzH8gZyA"
      },
      "source": [
        "# First, we'll import some handy data visualization tools\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPUsnMfOyr-F"
      },
      "source": [
        "Let's focus on one feature for now: mean radius. How well does it predict diagnosis?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXefT2nZgdnE"
      },
      "source": [
        "sns.catplot(x = 'radius_mean', y = 'diagnosis_cat', data = dataframe, order=['1 (malignant)', '0 (benign)'])\n",
        "dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_UCic0jgpWL"
      },
      "source": [
        "### üí° Discussion Question\n",
        "\n",
        "How would you interpret what is going on in the chart above?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution\n",
        "\n",
        "<details><summary> click to reveal! </summary>\n",
        "\n",
        "To some degree, lower mean radius is correlated with benign tumors and higher mean radius with malignant tumors. We expect this biologically -- since cancer is a disease of rapidly growing and dividing cells, we expect cells to be more deformed and to have more replicating (non-compact) DNA on average than healthy cells. Thus, a larger mean nucleus radius. That said, this isn't perfect; we see overlap between mean radius values and the two categories in the middle of the plot (investigated in the next section)."
      ],
      "metadata": {
        "id": "ll1KhaOr6T9X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w8-pnAy2rH7"
      },
      "source": [
        "### ‚úç Exercise\n",
        "\n",
        "Try out some other features (e.g. perimeter_mean, texture_mean, smoothness_mean) to see how they relate to the diagnosis. Which single feature seems like the best predictor?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG8x9O6Ohkri"
      },
      "source": [
        "# Predicting Diagnosis\n",
        "\n",
        "Let's start by predicting a diagnosis using a single feature: radius mean.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvPG8OMglv4U"
      },
      "source": [
        "## Approach 1: Can we use linear regression to classify these cells?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by using an algorithm that we've seen before: linear regression!\n",
        "\n",
        "### üí° Discussion Question\n",
        "\n",
        "How might linear regression be useful to classify examples from this dataset?"
      ],
      "metadata": {
        "id": "g8t5XB066p-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution\n",
        "<details><summary> click to reveal! </summary>\n",
        "\n",
        "This is a trick question. While you can perform linear regression on this data, it is not optimal for classification problems."
      ],
      "metadata": {
        "id": "bLEVjr4H6zC2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD0WAIB4l3p8",
        "cellView": "form"
      },
      "source": [
        "#@title Run this to fit and visualize a linear regression (double-click to see code!)\n",
        "from sklearn import linear_model\n",
        "\n",
        "X,y = dataframe[['radius_mean']], dataframe[['diagnosis']]\n",
        "\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(X, y)\n",
        "preds = model.predict(X)\n",
        "\n",
        "sns.scatterplot(x='radius_mean', y='diagnosis', data=dataframe)\n",
        "plt.plot(X, preds, color='r')\n",
        "plt.legend([ 'Data', 'Linear Regression Fit'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Take a look at the linear regression model and answer the following questions:\n",
        "\n",
        "#@markdown What does a diagnosis of 0.0 mean?\n",
        "diagnosis_0 = \"Benign\" #@param [\"Malignant\", \"Benign\", \"Choose An Answer\"]\n",
        "\n",
        "#@markdown What does a diagnosis of 1.0 mean?\n",
        "diagnosis_1 = \"Malignant\" #@param [\"Malignant\", \"Benign\", \"Choose An Answer\"]\n",
        "\n",
        "#@markdown What does the model predict for radius_mean = 20?\n",
        "radius_mean_20 = \"Malignant\" #@param [\"Malignant\", \"Benign\", \"Choose An Answer\"]\n",
        "\n",
        "#@markdown What does the model predict for radius_mean = 11?\n",
        "radius_mean_11 = \"Benign\" #@param [\"Malignant\", \"Benign\", \"Choose An Answer\"]\n",
        "\n",
        "if diagnosis_0 == 'Benign' and diagnosis_1 == 'Malignant':\n",
        "  print(\"Correct! 0.0 is a benign prediction and 1.0 is malignant.\")\n",
        "else:\n",
        "  print(\"One or both of our diagnoses' interpretations is incorrect. Try again!\")\n",
        "\n",
        "if radius_mean_20 == 'Malignant':\n",
        "  print(\"Correct! Our model would predict that a biopsy with radius_mean = 20 is malignant.\")\n",
        "else:\n",
        "  print(\"That's not quite what our model would predict for radius_mean = 20. Try again!\")\n",
        "\n",
        "if radius_mean_11 == 'Benign':\n",
        "  print(\"Correct! Our model would predict that a biopsy with radius_mean = 11 is benign.\")\n",
        "else:\n",
        "  print(\"That's not quite what our model would predict for radius_mean = 11. Try again!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GbTgO_3qPA_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQK_tn0xpJ7b"
      },
      "source": [
        "### üí° Discussion Question\n",
        "\n",
        "Did this linear regression model do well?\n",
        "\n",
        "**Hint**: What would our linear regression model predict for a mean radius of 25? How about 30? Is this an appropriate output?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution\n",
        "<details><summary> click to reveal! </summary>\n",
        "\n",
        "No. Only 0 and 1 are valid classifications. In this model the radius_mean would have to be 10 or 20 to be valid."
      ],
      "metadata": {
        "id": "ikLDgVo47nIt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85g7Jc9jh95i"
      },
      "source": [
        "##Approach 2: Classification - ¬†Simple Boundary Classifier\n",
        "The variable we are trying to predict is categorical, not continuous! So we can't use a linear regression; we have to use a classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFG1rBi8zq0n"
      },
      "source": [
        "### Classification is just drawing boundaries!\n",
        "\n",
        "The simplest approach to classification is just drawing a boundary. Let's pick a boundary value for the radius mean and see how well it separates the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oYBTJsvyaC6",
        "cellView": "form"
      },
      "source": [
        "#@title Choose a value for your boundary line and click play!\n",
        "\n",
        "#@markdown Double-click this cell to see the plotting code.\n",
        "target_boundary = 10 #@param {type:\"slider\", min:5, max:30, step:0.5}\n",
        "\n",
        "sns.catplot(x = 'radius_mean', y = 'diagnosis_cat', data = dataframe, order=['1 (malignant)', '0 (benign)'])\n",
        "plt.plot([target_boundary, target_boundary], [-.2, 1.2], 'g', linewidth = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXww4S6cNeo7"
      },
      "source": [
        "### üí° Discussion Question\n",
        "Does this boundary value separate the data well? What do the points in each part of the graph represent?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution\n",
        "<details><summary> click to reveal! </summary>\n",
        "\n",
        "Since there's overlap in the 12.5 to 17.5 region, we cannot achieve a perfect separation. This model would classify the points to the left of the boundary line as benign, and those to the right would be classified as malignant."
      ],
      "metadata": {
        "id": "we2do7gR8I9e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZmcQ_vGWZnR"
      },
      "source": [
        "### Building the boundary classifier\n",
        "\n",
        "Here we build a boundary classifier function that takes in a **target boundary**: a particular value of radius mean. This function will take in a boundary value of our choosing and then classify the data points based on whether or not they are above or below the boundary.\n",
        "\n",
        "**Exercise: Write a function to implement a boundary classifier.** You'll take in a `target_boundary` (a `float` or `int` like 15) and a `radius_mean_series` (a list of values) and return a list of predictions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Vc2zc5QTel4"
      },
      "source": [
        "def boundary_classifier(target_boundary, radius_mean_series):\n",
        "  predictions = []\n",
        "\n",
        "  for radius_mean in radius_mean_series:\n",
        "    if radius_mean > target_boundary:\n",
        "      predictions.append(1)\n",
        "    else:\n",
        "      pass # YOUR CODE HERE (delete the pass)\n",
        "\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_waD6mTKgge"
      },
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "def boundary_classifier(target_boundary, radius_mean_series):\n",
        "  predictions = []\n",
        "  for radius_mean in radius_mean_series:\n",
        "    if radius_mean > target_boundary:\n",
        "      predictions.append(1)\n",
        "    else:\n",
        "      predictions.append(0)\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBttJRoNYYdJ"
      },
      "source": [
        "The code below chooses a boundary and runs your classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PiRMX-xKjiA",
        "cellView": "form"
      },
      "source": [
        "#@title Choose a value for your boundary line and click play to see your classifier at work!\n",
        "\n",
        "#@markdown Double-click this cell to see the code for `y_pred` and `y_true`.\n",
        "chosen_boundary = 10 #@param {type:\"slider\", min:5, max:30, step:0.5}\n",
        "\n",
        "y_pred = boundary_classifier(chosen_boundary, dataframe['radius_mean'])\n",
        "dataframe['predicted'] = y_pred\n",
        "\n",
        "y_true = dataframe['diagnosis']\n",
        "\n",
        "sns.catplot(x = 'radius_mean', y = 'diagnosis_cat', hue = 'predicted', data = dataframe, order=['1 (malignant)', '0 (benign)'])\n",
        "plt.plot([chosen_boundary, chosen_boundary], [-.2, 1.2], 'g', linewidth = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esZdsGm69ZSV"
      },
      "source": [
        "What do you think of the results based on the graph?\n",
        "\n",
        "We can take a look at `y_true` and `y_pred` - how similar do they look?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZNaOVm-9cxb"
      },
      "source": [
        "print (list(y_true))\n",
        "print (y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-038BKC9ntz"
      },
      "source": [
        "Let's calculate our accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro6toAITfrp1"
      },
      "source": [
        "accuracy = accuracy_score(y_true,y_pred)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkVDw3wHZMD_"
      },
      "source": [
        "**Now adjust the chosen boundary above to get the best possible 'separation'.** As you do that, think about what it means for a separation to be 'good' - is it just the highest accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHhZeAbdupMl"
      },
      "source": [
        "##Approach 3: Logistic Regression - using machine learning to determine the optimal boundary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69c2JKp8Mxwi"
      },
      "source": [
        "\n",
        "Now, it's time to move away from our simple guess-and-check model and work towards implementing an approach that can automatically find a better separation. One of the most common methods for this is called 'Logistic Regression'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJfT1x60fZxd"
      },
      "source": [
        "### Training Data vs Test Data\n",
        "\n",
        "We'll split up our data set into groups called 'train' and 'test'. We teach our 'model' the patterns using the train data, but the whole point of machine learning is that our prediction should work on 'unseen' data or 'test' data.\n",
        "\n",
        "The function below does this for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krQ6dJo-5yek"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(dataframe, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOECnkKBf_tD"
      },
      "source": [
        "Let's now take a look at the 'train' and 'test' groups:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDYAAl6xgPYO"
      },
      "source": [
        "print('Number of rows in training dataframe:', train_df.shape[0])\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMornfXPhzli"
      },
      "source": [
        "print('Number of rows in test dataframe:', test_df.shape[0])\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WuOmB45g1-B"
      },
      "source": [
        "### Single Variable Logistic Regression\n",
        "To start with, let's set our input feature to be radius mean and our output variable to be the diagnosis.\n",
        "\n",
        "We will use this to build a logistic regression model to predict the diagnosis using radius mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chesGAaKNVON"
      },
      "source": [
        "X = ['radius_mean']\n",
        "y = 'diagnosis'\n",
        "\n",
        "X_train = train_df[X]\n",
        "print('X_train, our input variables:')\n",
        "print(X_train.head())\n",
        "print()\n",
        "\n",
        "y_train = train_df[y]\n",
        "print('y_train, our output variable:')\n",
        "print(y_train.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ChOPgKIi6pI"
      },
      "source": [
        "### üí° Discussion Question\n",
        "\n",
        "What's the difference between `X_train` and `y_train`?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution\n",
        "<details><summary> click to reveal! </summary>\n",
        "\n",
        "`X_train` contains the input numerical data for our model (in this case the radius_mean of our samples). `y_train` contains the output classification: malignant (1) or benign (0)"
      ],
      "metadata": {
        "id": "JSr9C6839sSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's prepare our model (we haven't trained it yet):"
      ],
      "metadata": {
        "id": "LTpjXV_T9oed"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj1LIpVT6SwY"
      },
      "source": [
        "# Here, we create a 'logreg_model' object that handles the line fitting for us!\n",
        "logreg_model = linear_model.LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shsboaDEjSC_"
      },
      "source": [
        "###Making Predictions\n",
        "\n",
        "Next, we want to tell our `logreg_model` object to take in our inputs (X) and our true labels (y) and fit a line that predicts y from X.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚úç Exercise\n",
        "Can you place the arguments `X_train` and `y_train` correctly into this function to do this?\n",
        "\n",
        "`logreg_model.fit(FILL_ME_IN, FILL_ME_IN)`"
      ],
      "metadata": {
        "id": "POoiGaiXKy6d"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfV0t3bqjtum"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fffn16PcNR6E"
      },
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "# ANSWER:\n",
        "logreg_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_keLGwQjzsu"
      },
      "source": [
        "### Testing our model\n",
        "\n",
        "How do we know if our 'model' is actually 'learning' anything? We need to test it on unseen data.\n",
        "\n",
        "Here we will be designating test inputs to check our model. Let's prepare the inputs and outputs from our testing dataset - try printing them out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaBE6yg6kF2N"
      },
      "source": [
        "X_test = test_df[X]\n",
        "y_test = test_df[y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Ii-5ijkX8K"
      },
      "source": [
        "### Making predictions on our test set\n",
        "\n",
        "Next, we need to figure out what our line thinks the diagnosis is based on our data points\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚úç Exercise\n",
        "\n",
        "Fill in the appropriate input to this function and run the function below.\n",
        "\n",
        "`y_pred = logreg_model.predict(FILL_ME_IN)`"
      ],
      "metadata": {
        "id": "z7p4ud6qK51c"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_59Wk6q46XVq"
      },
      "source": [
        "## YOUR CODE HERE\n",
        "\n",
        "## END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvAuNDRXNOzT"
      },
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "# ANSWER:\n",
        "\n",
        "y_pred = logreg_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ensyfit5lP1U"
      },
      "source": [
        "Run the code below to visualize the results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUSOLlz8lJu-"
      },
      "source": [
        "test_df['predicted'] = y_pred\n",
        "sns.catplot(x = 'radius_mean', y = 'diagnosis_cat', hue = 'predicted', data=test_df, order=['1 (malignant)', '0 (benign)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2fjWtn2aAVp"
      },
      "source": [
        "How does it look compared to the predictions before?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_DJgPAHlaVQ"
      },
      "source": [
        "### Finally, let's evaluate the accuracy of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKz5TfiOTHDl"
      },
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAi-LWA12aLh"
      },
      "source": [
        "## What is logistic regression doing? It's giving 'soft' predictions!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0tu5u7Y7CVL"
      },
      "source": [
        "#@title Run this to plot logistic regression's soft probabilities { display-mode: \"form\" }\n",
        "\n",
        "# Let's visualize the probabilities for `X_test`\n",
        "y_prob = logreg_model.predict_proba(X_test)\n",
        "X_test_view = X_test[X].values.squeeze()\n",
        "plt.xlabel('radius_mean')\n",
        "plt.ylabel('Predicted Probability')\n",
        "sns.scatterplot(x = X_test_view, y = y_prob[:,1], hue = y_test, palette=['purple','green'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2YecYWaN04j"
      },
      "source": [
        "The Y-axis is the  probability of being 'malignant' and the X-axis is the radius mean. The colors show the **true** diagnosis (this is different than previous graphs!)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üí° Discussion Question\n",
        "\n",
        "Can you interpret or take a guess about what the graph above is saying?"
      ],
      "metadata": {
        "id": "z8wXpwh2-qTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution\n",
        "<details><summary> click to reveal! </summary>\n",
        "\n",
        "To decide if the sample is 'malignant', it draws a 'decision boundary' where it thinks the sample is equally likely to be 'malignant' and 'normal', and asks 'am I to the left or the right of the boundary?'"
      ],
      "metadata": {
        "id": "1qj7iNmx-zUe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ9Bn0ol9L-6"
      },
      "source": [
        "# Approach 4: Multiple Feature Logistic Regression\n",
        "\n",
        "Which features best predict the diagnosis?\n",
        "\n",
        "Now that we can use logistic regression to find the optimal classification boundary, let's try out other features to see how well they predict the diagnosis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdezGrsP9_YA"
      },
      "source": [
        "First let's print out one row of our table so we can see what other features we have available to us.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB0iDuJ79_JE"
      },
      "source": [
        "dataframe.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pTHDxLrEQZK"
      },
      "source": [
        "### Experimenting with Single-Variable Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNdHjSV5-E6J"
      },
      "source": [
        "First, let's practice what we've done already! Fill in the code below to prepare your X and y data, fit the model on the training data, and predict on the test data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úç Exercise\n",
        "\n",
        "Once you have this code working, try replacing `radius_mean` with other features to see how well each feature predicts diagnosis!"
      ],
      "metadata": {
        "id": "MfqGFUir--Rz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-ccKkaw-ced"
      },
      "source": [
        "X = ['radius_mean'] #Try changing this later!\n",
        "y = 'diagnosis'\n",
        "\n",
        "# 1. Split data into train and test\n",
        "train_df, test_df = train_test_split(dataframe, test_size = 0.2, random_state = 1)\n",
        "\n",
        "# 2. Prepare your X_train, X_test, y_train, and y_test variables by extracting the appropriate columns:\n",
        "\n",
        "# 3. Initialize the model object\n",
        "\n",
        "# 4. Fit the model to the training data\n",
        "\n",
        "# 5. Use this trained model to predict on the test data\n",
        "\n",
        "# 6. Evaluate the accuracy by comparing to to the test labels and print out accuracy."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEMOF4Ys9U3_"
      },
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "\n",
        "X = ['radius_mean']\n",
        "y = 'diagnosis'\n",
        "\n",
        "# 1. Split data into train and test\n",
        "train_df, test_df = train_test_split(dataframe, test_size = 0.2, random_state = 1)\n",
        "\n",
        "# 2. Prepare your X_train, X_test, y_train, and y_test variables by extracting the appropriate columns:\n",
        "X_train, X_test = train_df[X], test_df[X]\n",
        "y_train, y_test = train_df[y], test_df[y]\n",
        "\n",
        "# 3. Initialize the model object\n",
        "model = linear_model.LogisticRegression()\n",
        "\n",
        "# 4. Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Use this trained model to predict on the test data\n",
        "preds = model.predict(X_test)\n",
        "\n",
        "# 6. Evaluate the accuracy by comparing to to the test labels and print out accuracy.\n",
        "accuracy = accuracy_score(y_test, preds)\n",
        "print(X[0], accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmmlzBQ0_BUU"
      },
      "source": [
        "###üí° Discussion Question\n",
        "\n",
        "Which features best predicted diagnosis? What does this teach us about breast cancer?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution\n",
        "<details><summary> click to reveal! </summary>\n",
        "\n",
        "From testing each feature, area_mean, radius_mean, and perimeter_mean all roughly have the highest accuracy at around 86.8%. This means that abnormally large cells in collected samples are likely indicators for breast cancer."
      ],
      "metadata": {
        "id": "UaYveFMt_RTP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7_d9NGB-WQm"
      },
      "source": [
        "## Can we use multiple features together to do even better?\n",
        "So far, we've just been using `radius_mean` to make predictions. But there's lots of other potentially important features that we could be using!\n",
        "\n",
        "Let's take a look again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbPQuFWE_btz"
      },
      "source": [
        "dataframe.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsqeWkykEK01"
      },
      "source": [
        "### Logistic Regression with Multiple Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33PSuuffA-zI"
      },
      "source": [
        "Now, let's try re-fitting the model using **your choice of multiple features.**\n",
        "\n",
        "Just add more features to the list: for example, to use two features you could have\n",
        "\n",
        "`X = ['radius_mean','area_mean']`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Instructor Discussion Point Solution:*** `area_mean`, `radius_mean`, `perimeter_mean` together won't affect the accuracy as opposed to just area_mean since correlation-wise they are very similar metrics. `area`, `radius`, and `perimeter` have the same impact."
      ],
      "metadata": {
        "id": "ELtb15_sk4Oi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsM-CLG6A2l8"
      },
      "source": [
        "X = [] # Add your features!\n",
        "y = 'diagnosis'\n",
        "\n",
        "# 1. Split data into train and test\n",
        "train_df, test_df = train_test_split(dataframe, test_size = 0.2, random_state = 1)\n",
        "\n",
        "# 2. Prepare your X_train, X_test, y_train, and y_test variables by extracting the appropriate columns:\n",
        "\n",
        "# 3. Initialize the model object\n",
        "\n",
        "# 4. Fit the model to the training data\n",
        "\n",
        "# 5. Use this trained model to predict on the test data\n",
        "\n",
        "# 6. Evaluate the accuracy by comparing to to the test labels and print out accuracy."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnXhkAAs-X8s"
      },
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "X = ['perimeter_mean', 'radius_mean', 'texture_mean','area_mean']\n",
        "y = 'diagnosis'\n",
        "\n",
        "# 1. Split data into train and test\n",
        "train_df, test_df = train_test_split(dataframe, test_size = 0.2, random_state = 1)\n",
        "\n",
        "# 2. Prepare your X_train, X_test, y_train, and y_test variables by extracting the appropriate columns:\n",
        "X_train, X_test = train_df[X], test_df[X]\n",
        "y_train, y_test = train_df[y], test_df[y]\n",
        "\n",
        "# 3. Initialize the model object\n",
        "model = linear_model.LogisticRegression()\n",
        "\n",
        "# 4. Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Use this trained model to predict on the test data\n",
        "preds = model.predict(X_test)\n",
        "\n",
        "# 6. Evaluate the accuracy by comparing to to the test labels and print out accuracy.\n",
        "accuracy = accuracy_score(y_test, preds)\n",
        "print(X)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUxvFalP_kcG"
      },
      "source": [
        "Logistic Regression can learn an optimal classification boundary by using multiple features together, which can improve its prediction accuracy even more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJdMjjFHMA22"
      },
      "source": [
        "# Bonus Discussion: What makes a separation good?\n",
        "\n",
        "We know our overall accuracy, so we know how many errors we make overall. Errors however come in two kinds:\n",
        "\n",
        "- **False positives:** The model predicts that a sample is malignant (positive), but it's actually benign.\n",
        "\n",
        "- **False negatives:** The model predicts that a sample is benign (negative), but it's actually malignant."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üí° Discussion Question\n",
        "\n",
        "In medical diagnoses, what are the dangers of each kind of mistake? What kind is worse? Can you think of an application where the opposite is true?\n",
        "\n",
        "A key insight is that there's a trade-off between the two kinds of errors! For example, how could you make a classifier that's guaranteed to have no false negatives? Would that be a good classifier?\n",
        "\n",
        "We have to find an acceptable balance!"
      ],
      "metadata": {
        "id": "DB1PgWEgBKDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution\n",
        "<details><summary>click to reveal!</summary>\n",
        "\n",
        "False positives could lead to individuals seeking treatment for cancer when they don't actually need it, while false negatives would lead some with cancer to believe they're healthy and not seek the treatment they need. The latter is arguably a worse situation.\n",
        "\n",
        "An application where a false negative could be preferred is when evaluating the efficacy of cancer drugs. One might prefer an effective drug (positive) to be mistakenly labeled ineffective (negative) over ineffective drugs being approved, needlessly taken, and potentially making the situation worse.\n",
        "\n",
        "A classifier that's guaranteed to have no false negatives is one that predicts everything as positive, which is definitely not a good classifier!"
      ],
      "metadata": {
        "id": "Ua6XfKYvBOmq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_cmSK_pbjvP"
      },
      "source": [
        "###Confusion Matrices\n",
        "Next, let's evaluate the performance of our model quantitatively. We can visualize statistics on the number of correct vs. incorrect predictions using a confusion matrix that shows the following:\n",
        "\n",
        "![Confusion Matrix](https://miro.medium.com/max/860/1*7EcPtd8DXu1ObPnZSukIdQ.png)\n",
        "\n",
        "where the terms mean:\n",
        "\n",
        "* **TP (True Positive)** = The model predicted positive (malignant in our case, since malignant has a label of 1) and it‚Äôs true.\n",
        "* **TN (True Negative)** = The model predicted negative (benign in our case, since benign has a label of 0) and it‚Äôs true.\n",
        "* **FP (False Positive)** = The model predicted positive and it‚Äôs false.\n",
        "* **FN (False Negative)** = The model predicted negative and it‚Äôs false."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcfsPpup9ljK"
      },
      "source": [
        "#@title Run this code to create a confusion matrix. { display-mode: \"form\" }\n",
        "#@markdown If you are curious how it works you may double-click to inspect the code.\n",
        "\n",
        "# Import the metrics class\n",
        "from sklearn import metrics\n",
        "\n",
        "# Create the Confusion Matrix\n",
        "# y_test = dataframe['diagnosis']\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualizing the Confusion Matrix\n",
        "class_names = [0,1] # Our diagnosis categories\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "# Setting up and visualizing the plot (do not worry about the code below!)\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g') # Creating heatmap\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix', y = 1.1)\n",
        "plt.ylabel('Actual diagnosis')\n",
        "plt.xlabel('Predicted diagnosis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Take a look at the confusion matrix and answer the following questions:\n",
        "\n",
        "#@markdown What are the values in the top left (0, 0)?\n",
        "top_left = \"Choose an Answer\" #@param [\"True Positives\", \"True Negatives\", \"False Positives\", \"False Negatives\", \"Choose an Answer\"]\n",
        "\n",
        "#@markdown What are the values in the bottom right (1, 1)?\n",
        "bottom_right = \"Choose an Answer\" #@param [\"True Positives\", \"True Negatives\", \"False Positives\", \"False Negatives\", \"Choose an Answer\"]\n",
        "\n",
        "#@markdown What are the values in the top right (1, 0)?\n",
        "top_right = \"Choose an Answer\" #@param [\"True Positives\", \"True Negatives\", \"False Positives\", \"False Negatives\", \"Choose an Answer\"]\n",
        "\n",
        "#@markdown What are the values in the bottom left (0, 1)?\n",
        "bottom_left = \"Choose an Answer\" #@param [\"True Positives\", \"True Negatives\", \"False Positives\", \"False Negatives\", \"Choose an Answer\"]\n",
        "\n",
        "if top_left == \"True Negatives\" and bottom_right == \"True Positives\":\n",
        "  print(\"Correct! Our results are True if our model is correct!\")\n",
        "else:\n",
        "  print(\"One or both of our (0, 0) and (1, 1) interpretations is incorrect. Try again!\")\n",
        "\n",
        "if top_right == \"False Positives\":\n",
        "  print(\"Correct! A false positive is when our model predicts that a sample is malignant when it's actually benign.\")\n",
        "else:\n",
        "  print(\"That's not quite what (1, 0) values are. Try again!\")\n",
        "\n",
        "if bottom_left == \"False Negatives\":\n",
        "  print(\"Correct! A false negative is when our model predicts that a sample is benign when it's actually malignant.\")\n",
        "else:\n",
        "  print(\"That's not quite what (0, 1) values are. Try again!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GIqCLiz1kqlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution\n",
        "<details><summary>click to reveal!</summary>\n",
        "\n",
        "- **top_left**: `True Negatives`\n",
        "- **bottom_right**: `True Positives`\n",
        "- **top_right**: `False Positives`\n",
        "- **bottom_left**: `False Negatives`"
      ],
      "metadata": {
        "id": "7a3VWT9CkXvL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzzrWDT_cxvF"
      },
      "source": [
        "### üí° Discussion Question\n",
        "- How many `True` values did our model predict?\n",
        "- How many `False` values?\n",
        "- Is our model a good classifier? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution\n",
        "<details><summary>click to reveal!</summary>\n",
        "\n",
        "- The model predicted 31 `True` values (where predicted diagnosis is 1, or the right column of the matrix)\n",
        "- There are 83 `False` predictions\n",
        "- It depends on which error you're trying to avoid! Our model correctly classifies $70/72 \\approx 97\\%$ of the healthy individuals and $29/42 \\approx 69\\%$ of the sick individuals. Based on the previous discussion, we'd probably prefer to identify more of the sick individuals!"
      ],
      "metadata": {
        "id": "N5wSUxMiHmct"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu4_0P0BfjYF"
      },
      "source": [
        "###Optional Challenge Exercise: Choosing a Metric\n",
        "\n",
        "Depending on the situation, we might measure success in different ways. For example, we might use:\n",
        "\n",
        "- **Accuracy:** What portion of our predictions are right?\n",
        "\n",
        "- **Precision:** What portion of our positive predictions are actually positive?\n",
        "\n",
        "- **Recall:** What portion of the actual positives did we identify?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###üí° Discussion Question\n",
        "\n",
        "Which metric is most important for cancer diagnosis?"
      ],
      "metadata": {
        "id": "fV-0DCXSI3rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution\n",
        "<details><summary>click to reveal!</summary>\n",
        "\n",
        "Each of these metrics has pros and cons. If we use a Clinical Reference Standard for labeling, which is a consensus of multiple doctors, generally accuracy and precision are very important. Accuracy is important since high accuracy corresponds to fewer False Negatives (which could be deadly in this case). High precision ensures that doctors are more accurate with their positives as well."
      ],
      "metadata": {
        "id": "D8Mxos02IqOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate any of these, we can use the numbers from our confusion matrix:"
      ],
      "metadata": {
        "id": "oZXRshFOI2EJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVejXKvcfitm"
      },
      "source": [
        "print (cnf_matrix)\n",
        "(tn, fp), (fn, tp) = cnf_matrix\n",
        "print (\"TN, FP, FN, TP:\", tn, fp, fn, tp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p140EnBtxL1F"
      },
      "source": [
        "Now, calculate your model's performance by your chosen metric! You can use the [table on Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix) to choose a metric and find a formula. How does it change your view of your model?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuZPSa-Jxgfs"
      },
      "source": [
        "#YOUR CODE HERE"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Y2gGHcxhlG",
        "cellView": "form"
      },
      "source": [
        "#@title Instructor Solution\n",
        "accuracy = (tp + tn)/(tn + fp + fn + tp)\n",
        "precision = (tp)/(tp + fp)\n",
        "recall = tp/(tp + fn)\n",
        "\n",
        "print (\"Accuracy:\", accuracy)\n",
        "print (\"Precision:\", precision)\n",
        "print (\"Recall:\", recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1aukjaVKHxx"
      },
      "source": [
        "**Congratulations!** You've successfully trained and evaluated a logistic regression model for diagnosing cancer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCP5dB9dvJ1x"
      },
      "source": [
        "#Optional: Decision Trees Walkthrough\n",
        "\n",
        "Finally, let's try a different classification model: decision trees! Recall that with decision trees, we choose features that create the best splits of our dataset (separates it into classes as best it can at that time)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbpWRKvyTedY"
      },
      "source": [
        "#@title Create the model { display-mode: \"both\" }\n",
        "from sklearn import tree\n",
        "\n",
        "# We'll first specify what model we want, in this case a decision tree\n",
        "class_dt = tree.DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "# We use our previous `X_train` and `y_train` sets to build the model\n",
        "class_dt.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PpoiLD77HkK"
      },
      "source": [
        "#@title Visualize and interpret the tree\n",
        "plt.figure(figsize=(13,8))  # set plot size\n",
        "tree.plot_tree(class_dt, fontsize=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKUiw9KdUQBs"
      },
      "source": [
        "#@title Find the predictions based on the model { display-mode: \"both\" }\n",
        "# now let's see how it performed!\n",
        "y_pred = class_dt.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcJmeOmFUaHP"
      },
      "source": [
        "#@title Calculate model performance { display-mode: \"both\" }\n",
        "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n",
        "print(\"Recall: \", metrics.recall_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E73C1cXI7e_N"
      },
      "source": [
        "###üí°Discussion Question\n",
        "\n",
        "What features are included in this classifier? How might you interpret this tree? Did this do better than the logistic regression?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution\n",
        "<details><summary>click to reveal!</summary>\n",
        "\n",
        "In this solution we are using 'perimeter_mean', 'radius_mean', 'texture_mean', 'area_mean'\n",
        "\n",
        "A breakdown of each level can be written out as:\n",
        "- Level 0 (root): If the perimeter mean is less than or equal to 98.775 move down to the left else move down to the right\n",
        "- Level 1\n",
        "  - Node 0 (leftmost): If perimeter mean is less than or equal to 89.95 and 98.775 move down to the left else move down to the right\n",
        "  - Node 1: If texture mean is less than or equal to 16.395 move down to the left else move down to the right\n",
        "- Level 2\n",
        "  - Node 0: If perimeter mean is less than or equal to 85.25 both options go to benign\n",
        "  - Node 1: If texture mean is less than or equal to 85.25 go to benign else go to malignant\n",
        "  - Node 2: If area mean is less than or equal to 999.05 go to benign else go to malignant\n",
        "  - Node 3: If perimeter mean is less than or equal to 108.85 both options go to malignant\n",
        "\n",
        "In this instance decision trees did slightly worse in all performance metrics than multi feature logistic regression."
      ],
      "metadata": {
        "id": "RDOJIfrVAgYR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_xQGU1B03sz"
      },
      "source": [
        "# Advanced (Optional): Choosing a Classifier\n",
        "We've studied two common classifiers, but many more are available. You can read about some of them [here](https://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/).\n",
        "\n",
        "Let's try to choose the overall best classifier for this dataset. Fill in the code below to:\n",
        "*   Use a for loop to train and evaluate each classifer in the list on our dataset.\n",
        "*   Calculate the precision, recall, and accuracy on the test set for each classifier, and store the results in a data frame so it's easy to analyze.\n",
        "*   Create plots to show the relationships between precision, accuracy, and recall and help you choose the \"best\" classifier.\n",
        "\n",
        "Then experiment with changing the hyperparameters (options) of each classifier - can you get even better results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxYta-NO1-vV",
        "cellView": "form"
      },
      "source": [
        "#@title Run this to import classifiers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel, RBF, WhiteKernel\n",
        "kernal = 1.0 * RBF(length_scale=1e-1, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel( noise_level=1e-2, noise_level_bounds=(1e-10, 1e1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNnm1_--unHR"
      },
      "source": [
        "#Once you've got your code working, try changing the hyperparameters of the classifiers\n",
        "#to see if you can get even better results.\n",
        "#Can you find out what the hyperparameters mean?\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel=\"linear\", C=0.025),\n",
        "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
        "    DecisionTreeClassifier(max_depth=5),\n",
        "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "    MLPClassifier(alpha=1, max_iter=1000),\n",
        "    AdaBoostClassifier(),\n",
        "    GaussianNB(),\n",
        "    QuadraticDiscriminantAnalysis()]\n",
        "\n",
        "\n",
        "#Use a for loop to train and test each classifier, and print the results\n",
        "#You might find the code above useful, as well as https://towardsdatascience.com/a-python-beginners-look-at-loc-part-2-bddef7dfa7f2 .\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END CODE ###\n",
        "\n",
        "\n",
        "\n",
        "#Using pyplot, show the relationships between precision, recall, and/or accuracy.\n",
        "#Tutorial here: https://matplotlib.org/tutorials/introductory/pyplot.html\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5rPpQo7PoaR",
        "cellView": "form"
      },
      "source": [
        "#@title Possible Solution\n",
        "#Once you've got your code working, try changing the hyperparameters of the classifiers\n",
        "#to see if you can get even better results.\n",
        "#Can you find out what the hyperparameters mean?\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel=\"linear\", C=0.025),\n",
        "    GaussianProcessClassifier(kernal),\n",
        "    DecisionTreeClassifier(max_depth=5),\n",
        "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "    MLPClassifier(alpha=1, max_iter=1000),\n",
        "    AdaBoostClassifier(),\n",
        "    GaussianNB(),\n",
        "    QuadraticDiscriminantAnalysis()]\n",
        "\n",
        "#You might find the code above useful, as well as https://towardsdatascience.com/a-python-beginners-look-at-loc-part-2-bddef7dfa7f2 .\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "for classifier in classifiers:\n",
        "  print(\"---------------\")\n",
        "  print(str(classifier) + '\\n')\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  acc = metrics.accuracy_score(y_test, y_pred)\n",
        "  prec = metrics.precision_score(y_test, y_pred)\n",
        "  rec = metrics.recall_score(y_test, y_pred)\n",
        "  accuracies.append(acc)\n",
        "  precisions.append(prec)\n",
        "  recalls.append(rec)\n",
        "  print(\"Accuracy: \", acc)\n",
        "  print(\"Precision: \", prec)\n",
        "  print(\"Recall: \", rec)\n",
        "\n",
        "  print(\"---------------\")\n",
        "\n",
        "### END CODE ###\n",
        "\n",
        "#Using pyplot, show the relationships between precision, recall, and/or accuracy.\n",
        "#Tutorial here: https://matplotlib.org/tutorials/introductory/pyplot.html\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "plt.plot(accuracies)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(precisions)\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(recalls)\n",
        "plt.ylabel(\"Recall\")\n",
        "plt.show()\n",
        "\n",
        "### END CODE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lr8SaCT2scN"
      },
      "source": [
        "**Think about:**\n",
        "*   Which classifier would you choose?\n",
        "*   What are the relationships among precision, recall, and accuracy? For this dataset, which is most important?\n",
        "*   Can you find more successful hyperparameters for each classifer?\n",
        "\n",
        "Your experiments will help you find a classifier that works very well on our test set. However, you're running a risk by doing so much manual fine-tuning: you might end up \"overfitting\" (on a more meta level) by choosing a classifier that works well on your test set, but might not work well on other data.\n",
        "\n",
        "That's why most machine learning projects actually use [*three* datasets](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7): a training set that we use to train each candidate model; a validation set that we use to evaluate each candidate model and choose the best one; and finally, a test set which we use only once, to report the overall performance of our project.\n",
        "\n",
        "\n"
      ]
    }
  ]
}